{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Transfer Learnign Model Keras\n",
    "\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dropout, Flatten, Dense\n",
    "from keras import applications\n",
    "from keras import backend as K\n",
    "from keras.optimizers import Adam\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_width, img_height = 224, 224\n",
    "\n",
    "#top_model_weights_path = 'bottleneck_fc_model.h5'\n",
    "train_data_dir = '/home/ec2-user/src/Data/ChestXray/chest_xray/train'\n",
    "validation_data_dir = '/home/ec2-user/src/Data/ChestXray/chest_xray/val'\n",
    "test_dir = '/home/ec2-user/src/Data/ChestXray/chest_xray/test'\n",
    "nb_train_samples = 5216\n",
    "nb_validation_samples = 16\n",
    "epochs = 100\n",
    "batch_size = 16\n",
    "nb_test_samples = 624\n",
    "test_batch_size = 16\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_16 (InputLayer)        (None, None, None, 3)     0         \n",
      "_________________________________________________________________\n",
      "block1_conv1 (Conv2D)        (None, None, None, 64)    1792      \n",
      "_________________________________________________________________\n",
      "block1_conv2 (Conv2D)        (None, None, None, 64)    36928     \n",
      "_________________________________________________________________\n",
      "block1_pool (MaxPooling2D)   (None, None, None, 64)    0         \n",
      "_________________________________________________________________\n",
      "block2_conv1 (Conv2D)        (None, None, None, 128)   73856     \n",
      "_________________________________________________________________\n",
      "block2_conv2 (Conv2D)        (None, None, None, 128)   147584    \n",
      "_________________________________________________________________\n",
      "block2_pool (MaxPooling2D)   (None, None, None, 128)   0         \n",
      "_________________________________________________________________\n",
      "block3_conv1 (Conv2D)        (None, None, None, 256)   295168    \n",
      "_________________________________________________________________\n",
      "block3_conv2 (Conv2D)        (None, None, None, 256)   590080    \n",
      "_________________________________________________________________\n",
      "block3_conv3 (Conv2D)        (None, None, None, 256)   590080    \n",
      "_________________________________________________________________\n",
      "block3_pool (MaxPooling2D)   (None, None, None, 256)   0         \n",
      "_________________________________________________________________\n",
      "block4_conv1 (Conv2D)        (None, None, None, 512)   1180160   \n",
      "_________________________________________________________________\n",
      "block4_conv2 (Conv2D)        (None, None, None, 512)   2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv3 (Conv2D)        (None, None, None, 512)   2359808   \n",
      "_________________________________________________________________\n",
      "block4_pool (MaxPooling2D)   (None, None, None, 512)   0         \n",
      "_________________________________________________________________\n",
      "block5_conv1 (Conv2D)        (None, None, None, 512)   2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv2 (Conv2D)        (None, None, None, 512)   2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv3 (Conv2D)        (None, None, None, 512)   2359808   \n",
      "_________________________________________________________________\n",
      "block5_pool (MaxPooling2D)   (None, None, None, 512)   0         \n",
      "=================================================================\n",
      "Total params: 14,714,688\n",
      "Trainable params: 14,714,688\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = applications.VGG16(include_top=False, weights='imagenet')\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 5216 images belonging to 2 classes.\n",
      "Found 16 images belonging to 2 classes.\n",
      "Found 624 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "# reading in image data from training and validation steps \n",
    "\n",
    "# use ImageDataGenerator to normalize/augment \n",
    "\n",
    "datagen = ImageDataGenerator(rescale=1. / 255)\n",
    "\n",
    "# use flow_from_directory method to resize/pre-process \n",
    "\n",
    "generator = datagen.flow_from_directory(\n",
    "        train_data_dir,\n",
    "        target_size=(img_width, img_height),\n",
    "        batch_size=batch_size,\n",
    "        class_mode=None,\n",
    "        shuffle=False)\n",
    "\n",
    "#use model predict to create bottleneck weights from VGG16 & save to .npy file\n",
    "\n",
    "bottleneck_features_train = model.predict_generator(\n",
    "        generator, nb_train_samples // batch_size)\n",
    "np.save(open('bottleneck_features_train.npy', 'wb'),\n",
    "            bottleneck_features_train)\n",
    "\n",
    "generator = datagen.flow_from_directory(\n",
    "        validation_data_dir,\n",
    "        target_size=(img_width, img_height),\n",
    "        batch_size=batch_size,\n",
    "        class_mode=None,\n",
    "        shuffle=False)\n",
    "\n",
    "bottleneck_features_validation = model.predict_generator(\n",
    "        generator, nb_validation_samples // batch_size)\n",
    "np.save(open('bottleneck_features_validation.npy', 'wb'), bottleneck_features_validation)\n",
    "\n",
    "generator = datagen.flow_from_directory(\n",
    "        test_dir,\n",
    "        target_size=(img_width, img_height),\n",
    "        batch_size=batch_size,\n",
    "        class_mode=None,\n",
    "        shuffle=False)\n",
    "\n",
    "bottleneck_features_test = model.predict_generator(\n",
    "        generator, nb_test_samples // batch_size)\n",
    "np.save(open('bottleneck_features_test.npy', 'wb'), bottleneck_features_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train and create labels in sequential order\n",
    "\n",
    "# Normal 0; Pneumonia 1 \n",
    "\n",
    "train_data = np.load(open('bottleneck_features_train.npy','rb'))\n",
    "train_labels = np.array([0] * (1341) + [1] * (3875))\n",
    "\n",
    "validation_data = np.load(open('bottleneck_features_validation.npy','rb'))\n",
    "validation_labels = np.array([0] * (8) + [1] * (8))\n",
    "\n",
    "test_data = np.load(open('bottleneck_features_test.npy','rb'))\n",
    "test_labels = np.array([0] * (234) + [1] * (390))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5216, 7, 7, 512)\n",
      "(5216,)\n",
      "(16, 7, 7, 512)\n",
      "(16,)\n",
      "(624, 7, 7, 512)\n",
      "(624,)\n"
     ]
    }
   ],
   "source": [
    "print(train_data.shape)\n",
    "print(train_labels.shape)\n",
    "print(validation_data.shape)\n",
    "print(validation_labels.shape)\n",
    "\n",
    "print(test_data.shape)\n",
    "print(test_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten_25 (Flatten)         (None, 25088)             0         \n",
      "_________________________________________________________________\n",
      "dense_69 (Dense)             (None, 500)               12544500  \n",
      "_________________________________________________________________\n",
      "dropout_45 (Dropout)         (None, 500)               0         \n",
      "_________________________________________________________________\n",
      "dense_70 (Dense)             (None, 250)               125250    \n",
      "_________________________________________________________________\n",
      "dropout_46 (Dropout)         (None, 250)               0         \n",
      "_________________________________________________________________\n",
      "dense_71 (Dense)             (None, 1)                 251       \n",
      "=================================================================\n",
      "Total params: 12,670,001\n",
      "Trainable params: 12,670,001\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# create dense fully connected model to add to base pre-configured model \n",
    "# inputs from trained bottled features\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Flatten(input_shape=train_data.shape[1:]))\n",
    "model.add(Dense(500, activation='relu'))\n",
    "model.add(Dropout(0.6))\n",
    "model.add(Dense(250, activation='relu'))\n",
    "model.add(Dropout(0.6))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.summary()\n",
    "\n",
    "adam = Adam(lr=0.0001, decay=0.0001)\n",
    "model.compile(optimizer=adam, loss='binary_crossentropy', metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 5216 samples, validate on 16 samples\n",
      "Epoch 1/100\n",
      "5200/5216 [============================>.] - ETA: 0s - loss: 0.2531 - acc: 0.8929Epoch 00000: val_loss improved from inf to 0.42927, saving model to model.weights.best.hdf5\n",
      "5216/5216 [==============================] - 9s - loss: 0.2531 - acc: 0.8928 - val_loss: 0.4293 - val_acc: 0.7500\n",
      "Epoch 2/100\n",
      "5200/5216 [============================>.] - ETA: 0s - loss: 0.1114 - acc: 0.9598Epoch 00001: val_loss did not improve\n",
      "5216/5216 [==============================] - 6s - loss: 0.1120 - acc: 0.9595 - val_loss: 0.7281 - val_acc: 0.6875\n",
      "Epoch 3/100\n",
      "5200/5216 [============================>.] - ETA: 0s - loss: 0.0928 - acc: 0.9685Epoch 00002: val_loss improved from 0.42927 to 0.42017, saving model to model.weights.best.hdf5\n",
      "5216/5216 [==============================] - 7s - loss: 0.0936 - acc: 0.9682 - val_loss: 0.4202 - val_acc: 0.7500\n",
      "Epoch 4/100\n",
      "5200/5216 [============================>.] - ETA: 0s - loss: 0.0811 - acc: 0.9713Epoch 00003: val_loss did not improve\n",
      "5216/5216 [==============================] - 6s - loss: 0.0809 - acc: 0.9714 - val_loss: 0.4227 - val_acc: 0.8750\n",
      "Epoch 5/100\n",
      "5200/5216 [============================>.] - ETA: 0s - loss: 0.0646 - acc: 0.9752Epoch 00004: val_loss improved from 0.42017 to 0.34369, saving model to model.weights.best.hdf5\n",
      "5216/5216 [==============================] - 7s - loss: 0.0645 - acc: 0.9753 - val_loss: 0.3437 - val_acc: 0.8750\n",
      "Epoch 6/100\n",
      "5200/5216 [============================>.] - ETA: 0s - loss: 0.0588 - acc: 0.9796Epoch 00005: val_loss did not improve\n",
      "5216/5216 [==============================] - 6s - loss: 0.0590 - acc: 0.9795 - val_loss: 0.8151 - val_acc: 0.6875\n",
      "Epoch 7/100\n",
      "5200/5216 [============================>.] - ETA: 0s - loss: 0.0509 - acc: 0.9810Epoch 00006: val_loss improved from 0.34369 to 0.32775, saving model to model.weights.best.hdf5\n",
      "5216/5216 [==============================] - 7s - loss: 0.0511 - acc: 0.9808 - val_loss: 0.3278 - val_acc: 0.8750\n",
      "Epoch 8/100\n",
      "5200/5216 [============================>.] - ETA: 0s - loss: 0.0505 - acc: 0.9831Epoch 00007: val_loss improved from 0.32775 to 0.20259, saving model to model.weights.best.hdf5\n",
      "5216/5216 [==============================] - 7s - loss: 0.0505 - acc: 0.9831 - val_loss: 0.2026 - val_acc: 0.8750\n",
      "Epoch 9/100\n",
      "5200/5216 [============================>.] - ETA: 0s - loss: 0.0443 - acc: 0.9865Epoch 00008: val_loss did not improve\n",
      "5216/5216 [==============================] - 6s - loss: 0.0442 - acc: 0.9866 - val_loss: 0.4030 - val_acc: 0.8750\n",
      "Epoch 10/100\n",
      "5200/5216 [============================>.] - ETA: 0s - loss: 0.0367 - acc: 0.9875Epoch 00009: val_loss did not improve\n",
      "5216/5216 [==============================] - 6s - loss: 0.0367 - acc: 0.9875 - val_loss: 0.5736 - val_acc: 0.8125\n",
      "Epoch 11/100\n",
      "5200/5216 [============================>.] - ETA: 0s - loss: 0.0349 - acc: 0.9873Epoch 00010: val_loss did not improve\n",
      "5216/5216 [==============================] - 6s - loss: 0.0348 - acc: 0.9873 - val_loss: 0.2337 - val_acc: 0.8750\n",
      "Epoch 12/100\n",
      "5200/5216 [============================>.] - ETA: 0s - loss: 0.0350 - acc: 0.9888Epoch 00011: val_loss improved from 0.20259 to 0.17305, saving model to model.weights.best.hdf5\n",
      "5216/5216 [==============================] - 7s - loss: 0.0351 - acc: 0.9887 - val_loss: 0.1730 - val_acc: 0.8750\n",
      "Epoch 13/100\n",
      "5200/5216 [============================>.] - ETA: 0s - loss: 0.0305 - acc: 0.9892Epoch 00012: val_loss did not improve\n",
      "5216/5216 [==============================] - 6s - loss: 0.0308 - acc: 0.9891 - val_loss: 0.6108 - val_acc: 0.7500\n",
      "Epoch 14/100\n",
      "5200/5216 [============================>.] - ETA: 0s - loss: 0.0287 - acc: 0.9900Epoch 00013: val_loss did not improve\n",
      "5216/5216 [==============================] - 6s - loss: 0.0287 - acc: 0.9900 - val_loss: 0.2108 - val_acc: 0.8750\n",
      "Epoch 15/100\n",
      "5200/5216 [============================>.] - ETA: 0s - loss: 0.0263 - acc: 0.9910Epoch 00014: val_loss did not improve\n",
      "5216/5216 [==============================] - 6s - loss: 0.0263 - acc: 0.9910 - val_loss: 0.2972 - val_acc: 0.8750\n",
      "Epoch 16/100\n",
      "5200/5216 [============================>.] - ETA: 0s - loss: 0.0243 - acc: 0.9913Epoch 00015: val_loss improved from 0.17305 to 0.16452, saving model to model.weights.best.hdf5\n",
      "5216/5216 [==============================] - 7s - loss: 0.0243 - acc: 0.9914 - val_loss: 0.1645 - val_acc: 0.8750\n",
      "Epoch 17/100\n",
      "5200/5216 [============================>.] - ETA: 0s - loss: 0.0246 - acc: 0.9906Epoch 00016: val_loss did not improve\n",
      "5216/5216 [==============================] - 6s - loss: 0.0246 - acc: 0.9906 - val_loss: 0.4297 - val_acc: 0.8750\n",
      "Epoch 18/100\n",
      "5200/5216 [============================>.] - ETA: 0s - loss: 0.0188 - acc: 0.9925Epoch 00017: val_loss did not improve\n",
      "5216/5216 [==============================] - 6s - loss: 0.0188 - acc: 0.9925 - val_loss: 0.3006 - val_acc: 0.8750\n",
      "Epoch 19/100\n",
      "5200/5216 [============================>.] - ETA: 0s - loss: 0.0137 - acc: 0.9962Epoch 00018: val_loss did not improve\n",
      "5216/5216 [==============================] - 6s - loss: 0.0137 - acc: 0.9962 - val_loss: 0.2617 - val_acc: 0.8750\n",
      "Epoch 20/100\n",
      "5200/5216 [============================>.] - ETA: 0s - loss: 0.0197 - acc: 0.9937Epoch 00019: val_loss did not improve\n",
      "5216/5216 [==============================] - 6s - loss: 0.0201 - acc: 0.9935 - val_loss: 0.1724 - val_acc: 0.8750\n",
      "Epoch 21/100\n",
      "5200/5216 [============================>.] - ETA: 0s - loss: 0.0141 - acc: 0.9952Epoch 00020: val_loss did not improve\n",
      "5216/5216 [==============================] - 6s - loss: 0.0140 - acc: 0.9952 - val_loss: 0.3426 - val_acc: 0.8750\n",
      "Epoch 22/100\n",
      "5200/5216 [============================>.] - ETA: 0s - loss: 0.0133 - acc: 0.9948Epoch 00021: val_loss did not improve\n",
      "5216/5216 [==============================] - 6s - loss: 0.0133 - acc: 0.9948 - val_loss: 0.2716 - val_acc: 0.8750\n",
      "Epoch 23/100\n",
      "5200/5216 [============================>.] - ETA: 0s - loss: 0.0208 - acc: 0.9917Epoch 00022: val_loss did not improve\n",
      "5216/5216 [==============================] - 6s - loss: 0.0208 - acc: 0.9918 - val_loss: 0.3717 - val_acc: 0.8750\n",
      "Epoch 24/100\n",
      "5200/5216 [============================>.] - ETA: 0s - loss: 0.0115 - acc: 0.9963Epoch 00023: val_loss did not improve\n",
      "5216/5216 [==============================] - 6s - loss: 0.0115 - acc: 0.9964 - val_loss: 0.2571 - val_acc: 0.8750\n",
      "Epoch 25/100\n",
      "5200/5216 [============================>.] - ETA: 0s - loss: 0.0099 - acc: 0.9963Epoch 00024: val_loss did not improve\n",
      "5216/5216 [==============================] - 6s - loss: 0.0098 - acc: 0.9964 - val_loss: 0.3900 - val_acc: 0.8750\n",
      "Epoch 26/100\n",
      "5200/5216 [============================>.] - ETA: 0s - loss: 0.0094 - acc: 0.9969Epoch 00025: val_loss did not improve\n",
      "5216/5216 [==============================] - 6s - loss: 0.0093 - acc: 0.9969 - val_loss: 0.3206 - val_acc: 0.8750\n",
      "Epoch 27/100\n",
      "5200/5216 [============================>.] - ETA: 0s - loss: 0.0102 - acc: 0.9962Epoch 00026: val_loss did not improve\n",
      "5216/5216 [==============================] - 6s - loss: 0.0102 - acc: 0.9962 - val_loss: 0.2890 - val_acc: 0.8750\n",
      "Epoch 28/100\n",
      "5200/5216 [============================>.] - ETA: 0s - loss: 0.0083 - acc: 0.9973Epoch 00027: val_loss did not improve\n",
      "5216/5216 [==============================] - 6s - loss: 0.0082 - acc: 0.9973 - val_loss: 0.4756 - val_acc: 0.8750\n",
      "Epoch 29/100\n",
      "5200/5216 [============================>.] - ETA: 0s - loss: 0.0078 - acc: 0.9973Epoch 00028: val_loss did not improve\n",
      "5216/5216 [==============================] - 6s - loss: 0.0077 - acc: 0.9973 - val_loss: 0.3678 - val_acc: 0.8750\n",
      "Epoch 30/100\n",
      "5200/5216 [============================>.] - ETA: 0s - loss: 0.0109 - acc: 0.9967Epoch 00029: val_loss did not improve\n",
      "5216/5216 [==============================] - 6s - loss: 0.0109 - acc: 0.9967 - val_loss: 0.3529 - val_acc: 0.8750\n",
      "Epoch 31/100\n",
      "5200/5216 [============================>.] - ETA: 0s - loss: 0.0108 - acc: 0.9962Epoch 00030: val_loss did not improve\n",
      "5216/5216 [==============================] - 6s - loss: 0.0107 - acc: 0.9962 - val_loss: 0.1883 - val_acc: 0.8750\n",
      "Epoch 32/100\n",
      "5200/5216 [============================>.] - ETA: 0s - loss: 0.0076 - acc: 0.9975Epoch 00031: val_loss improved from 0.16452 to 0.03171, saving model to model.weights.best.hdf5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5216/5216 [==============================] - 7s - loss: 0.0076 - acc: 0.9975 - val_loss: 0.0317 - val_acc: 1.0000\n",
      "Epoch 33/100\n",
      "5200/5216 [============================>.] - ETA: 0s - loss: 0.0074 - acc: 0.9981Epoch 00032: val_loss did not improve\n",
      "5216/5216 [==============================] - 6s - loss: 0.0074 - acc: 0.9981 - val_loss: 0.0623 - val_acc: 1.0000\n",
      "Epoch 34/100\n",
      "5200/5216 [============================>.] - ETA: 0s - loss: 0.0059 - acc: 0.9979Epoch 00033: val_loss did not improve\n",
      "5216/5216 [==============================] - 6s - loss: 0.0058 - acc: 0.9979 - val_loss: 0.1676 - val_acc: 0.8750\n",
      "Epoch 35/100\n",
      "5200/5216 [============================>.] - ETA: 0s - loss: 0.0035 - acc: 0.9988Epoch 00034: val_loss did not improve\n",
      "5216/5216 [==============================] - 6s - loss: 0.0035 - acc: 0.9988 - val_loss: 0.2112 - val_acc: 0.8750\n",
      "Epoch 36/100\n",
      "5200/5216 [============================>.] - ETA: 0s - loss: 0.0103 - acc: 0.9967Epoch 00035: val_loss did not improve\n",
      "5216/5216 [==============================] - 6s - loss: 0.0103 - acc: 0.9967 - val_loss: 0.2986 - val_acc: 0.8750\n",
      "Epoch 37/100\n",
      "5200/5216 [============================>.] - ETA: 0s - loss: 0.0056 - acc: 0.9983Epoch 00036: val_loss did not improve\n",
      "5216/5216 [==============================] - 6s - loss: 0.0056 - acc: 0.9983 - val_loss: 0.0734 - val_acc: 0.9375\n",
      "Epoch 38/100\n",
      "5200/5216 [============================>.] - ETA: 0s - loss: 0.0082 - acc: 0.9969Epoch 00037: val_loss did not improve\n",
      "5216/5216 [==============================] - 6s - loss: 0.0082 - acc: 0.9969 - val_loss: 0.1699 - val_acc: 0.8750\n",
      "Epoch 39/100\n",
      "5200/5216 [============================>.] - ETA: 0s - loss: 0.0044 - acc: 0.9987Epoch 00038: val_loss did not improve\n",
      "5216/5216 [==============================] - 6s - loss: 0.0044 - acc: 0.9987 - val_loss: 0.1633 - val_acc: 0.8750\n",
      "Epoch 40/100\n",
      "5200/5216 [============================>.] - ETA: 0s - loss: 0.0027 - acc: 0.9994Epoch 00039: val_loss did not improve\n",
      "5216/5216 [==============================] - 6s - loss: 0.0027 - acc: 0.9994 - val_loss: 0.1931 - val_acc: 0.8750\n",
      "Epoch 41/100\n",
      "5200/5216 [============================>.] - ETA: 0s - loss: 0.0038 - acc: 0.9988Epoch 00040: val_loss did not improve\n",
      "5216/5216 [==============================] - 6s - loss: 0.0038 - acc: 0.9988 - val_loss: 0.1897 - val_acc: 0.8750\n",
      "Epoch 42/100\n",
      "5200/5216 [============================>.] - ETA: 0s - loss: 0.0037 - acc: 0.9985Epoch 00041: val_loss did not improve\n",
      "5216/5216 [==============================] - 6s - loss: 0.0037 - acc: 0.9985 - val_loss: 0.2649 - val_acc: 0.8750\n",
      "Epoch 43/100\n",
      "5200/5216 [============================>.] - ETA: 0s - loss: 0.0063 - acc: 0.9975Epoch 00042: val_loss did not improve\n",
      "5216/5216 [==============================] - 6s - loss: 0.0063 - acc: 0.9975 - val_loss: 0.0595 - val_acc: 0.9375\n",
      "Epoch 44/100\n",
      "5200/5216 [============================>.] - ETA: 0s - loss: 0.0036 - acc: 0.9987Epoch 00043: val_loss did not improve\n",
      "5216/5216 [==============================] - 6s - loss: 0.0036 - acc: 0.9987 - val_loss: 0.3337 - val_acc: 0.8750\n",
      "Epoch 45/100\n",
      "5200/5216 [============================>.] - ETA: 0s - loss: 0.0057 - acc: 0.9979Epoch 00044: val_loss did not improve\n",
      "5216/5216 [==============================] - 6s - loss: 0.0056 - acc: 0.9979 - val_loss: 0.2757 - val_acc: 0.8750\n",
      "Epoch 46/100\n",
      "5200/5216 [============================>.] - ETA: 0s - loss: 0.0032 - acc: 0.9992Epoch 00045: val_loss did not improve\n",
      "5216/5216 [==============================] - 6s - loss: 0.0032 - acc: 0.9992 - val_loss: 0.1415 - val_acc: 0.8750\n",
      "Epoch 47/100\n",
      "5200/5216 [============================>.] - ETA: 0s - loss: 0.0054 - acc: 0.9985Epoch 00046: val_loss did not improve\n",
      "5216/5216 [==============================] - 6s - loss: 0.0053 - acc: 0.9985 - val_loss: 0.1683 - val_acc: 0.8750\n",
      "Epoch 48/100\n",
      "5200/5216 [============================>.] - ETA: 0s - loss: 0.0061 - acc: 0.9979Epoch 00047: val_loss did not improve\n",
      "5216/5216 [==============================] - 6s - loss: 0.0061 - acc: 0.9979 - val_loss: 0.1357 - val_acc: 0.8750\n",
      "Epoch 49/100\n",
      "5200/5216 [============================>.] - ETA: 0s - loss: 0.0021 - acc: 0.9998Epoch 00048: val_loss did not improve\n",
      "5216/5216 [==============================] - 6s - loss: 0.0021 - acc: 0.9998 - val_loss: 0.0525 - val_acc: 1.0000\n",
      "Epoch 50/100\n",
      "5200/5216 [============================>.] - ETA: 0s - loss: 0.0033 - acc: 0.9994Epoch 00049: val_loss did not improve\n",
      "5216/5216 [==============================] - 6s - loss: 0.0032 - acc: 0.9994 - val_loss: 0.2257 - val_acc: 0.8750\n",
      "Epoch 51/100\n",
      "5200/5216 [============================>.] - ETA: 0s - loss: 0.0043 - acc: 0.9983Epoch 00050: val_loss did not improve\n",
      "5216/5216 [==============================] - 6s - loss: 0.0043 - acc: 0.9983 - val_loss: 0.3194 - val_acc: 0.8750\n",
      "Epoch 52/100\n",
      "5200/5216 [============================>.] - ETA: 0s - loss: 0.0036 - acc: 0.9987Epoch 00051: val_loss did not improve\n",
      "5216/5216 [==============================] - 6s - loss: 0.0036 - acc: 0.9987 - val_loss: 0.1884 - val_acc: 0.8750\n",
      "Epoch 53/100\n",
      "5200/5216 [============================>.] - ETA: 0s - loss: 0.0017 - acc: 0.9994Epoch 00052: val_loss did not improve\n",
      "5216/5216 [==============================] - 6s - loss: 0.0017 - acc: 0.9994 - val_loss: 0.2644 - val_acc: 0.8750\n",
      "Epoch 54/100\n",
      "5200/5216 [============================>.] - ETA: 0s - loss: 0.0023 - acc: 0.9988Epoch 00053: val_loss did not improve\n",
      "5216/5216 [==============================] - 6s - loss: 0.0024 - acc: 0.9988 - val_loss: 0.6940 - val_acc: 0.8750\n",
      "Epoch 55/100\n",
      "5200/5216 [============================>.] - ETA: 0s - loss: 0.0041 - acc: 0.9985Epoch 00054: val_loss did not improve\n",
      "5216/5216 [==============================] - 6s - loss: 0.0041 - acc: 0.9985 - val_loss: 0.6050 - val_acc: 0.8750\n",
      "Epoch 56/100\n",
      "5200/5216 [============================>.] - ETA: 0s - loss: 0.0023 - acc: 0.9992Epoch 00055: val_loss did not improve\n",
      "5216/5216 [==============================] - 6s - loss: 0.0023 - acc: 0.9992 - val_loss: 0.4693 - val_acc: 0.8750\n",
      "Epoch 57/100\n",
      "5200/5216 [============================>.] - ETA: 0s - loss: 0.0018 - acc: 0.9994Epoch 00056: val_loss did not improve\n",
      "5216/5216 [==============================] - 6s - loss: 0.0018 - acc: 0.9994 - val_loss: 0.4551 - val_acc: 0.8750\n",
      "Epoch 58/100\n",
      "5200/5216 [============================>.] - ETA: 0s - loss: 0.0031 - acc: 0.9990Epoch 00057: val_loss did not improve\n",
      "5216/5216 [==============================] - 6s - loss: 0.0031 - acc: 0.9990 - val_loss: 0.1711 - val_acc: 0.8750\n",
      "Epoch 59/100\n",
      "5200/5216 [============================>.] - ETA: 0s - loss: 9.2414e-04 - acc: 0.9998Epoch 00058: val_loss did not improve\n",
      "5216/5216 [==============================] - 6s - loss: 9.2145e-04 - acc: 0.9998 - val_loss: 0.4414 - val_acc: 0.8750\n",
      "Epoch 60/100\n",
      "5200/5216 [============================>.] - ETA: 0s - loss: 0.0074 - acc: 0.9967Epoch 00059: val_loss did not improve\n",
      "5216/5216 [==============================] - 6s - loss: 0.0073 - acc: 0.9967 - val_loss: 0.2734 - val_acc: 0.8750\n",
      "Epoch 61/100\n",
      "5200/5216 [============================>.] - ETA: 0s - loss: 0.0017 - acc: 0.9996Epoch 00060: val_loss did not improve\n",
      "5216/5216 [==============================] - 6s - loss: 0.0017 - acc: 0.9996 - val_loss: 0.0740 - val_acc: 0.9375\n",
      "Epoch 62/100\n",
      "5200/5216 [============================>.] - ETA: 0s - loss: 0.0012 - acc: 0.9996Epoch 00061: val_loss did not improve\n",
      "5216/5216 [==============================] - 6s - loss: 0.0012 - acc: 0.9996 - val_loss: 0.3370 - val_acc: 0.8750\n",
      "Epoch 63/100\n",
      "5200/5216 [============================>.] - ETA: 0s - loss: 4.5518e-04 - acc: 1.0000Epoch 00062: val_loss did not improve\n",
      "5216/5216 [==============================] - 6s - loss: 4.5379e-04 - acc: 1.0000 - val_loss: 0.3893 - val_acc: 0.8750\n",
      "Epoch 64/100\n",
      "5200/5216 [============================>.] - ETA: 0s - loss: 8.9131e-04 - acc: 1.0000Epoch 00063: val_loss did not improve\n",
      "5216/5216 [==============================] - 6s - loss: 8.8862e-04 - acc: 1.0000 - val_loss: 0.2607 - val_acc: 0.8750\n",
      "Epoch 65/100\n",
      "5200/5216 [============================>.] - ETA: 0s - loss: 0.0011 - acc: 0.9998Epoch 00064: val_loss did not improve\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5216/5216 [==============================] - 6s - loss: 0.0011 - acc: 0.9998 - val_loss: 0.7466 - val_acc: 0.8750\n",
      "Epoch 66/100\n",
      "5200/5216 [============================>.] - ETA: 0s - loss: 9.2563e-04 - acc: 0.9998Epoch 00065: val_loss improved from 0.03171 to 0.02124, saving model to model.weights.best.hdf5\n",
      "5216/5216 [==============================] - 7s - loss: 9.2758e-04 - acc: 0.9998 - val_loss: 0.0212 - val_acc: 1.0000\n",
      "Epoch 67/100\n",
      "5200/5216 [============================>.] - ETA: 0s - loss: 0.0043 - acc: 0.9983Epoch 00066: val_loss did not improve\n",
      "5216/5216 [==============================] - 6s - loss: 0.0043 - acc: 0.9983 - val_loss: 0.3574 - val_acc: 0.8750\n",
      "Epoch 68/100\n",
      "5200/5216 [============================>.] - ETA: 0s - loss: 0.0014 - acc: 0.9998Epoch 00067: val_loss did not improve\n",
      "5216/5216 [==============================] - 6s - loss: 0.0014 - acc: 0.9998 - val_loss: 0.5709 - val_acc: 0.8750\n",
      "Epoch 69/100\n",
      "5200/5216 [============================>.] - ETA: 0s - loss: 0.0026 - acc: 0.9990Epoch 00068: val_loss did not improve\n",
      "5216/5216 [==============================] - 6s - loss: 0.0026 - acc: 0.9990 - val_loss: 0.1028 - val_acc: 0.9375\n",
      "Epoch 70/100\n",
      "5200/5216 [============================>.] - ETA: 0s - loss: 0.0020 - acc: 0.9992Epoch 00069: val_loss did not improve\n",
      "5216/5216 [==============================] - 6s - loss: 0.0020 - acc: 0.9992 - val_loss: 0.8234 - val_acc: 0.8750\n",
      "Epoch 71/100\n",
      "5200/5216 [============================>.] - ETA: 0s - loss: 0.0028 - acc: 0.9990Epoch 00070: val_loss did not improve\n",
      "5216/5216 [==============================] - 6s - loss: 0.0028 - acc: 0.9990 - val_loss: 0.2135 - val_acc: 0.8750\n",
      "Epoch 72/100\n",
      "5200/5216 [============================>.] - ETA: 0s - loss: 0.0020 - acc: 0.9994Epoch 00071: val_loss did not improve\n",
      "5216/5216 [==============================] - 6s - loss: 0.0020 - acc: 0.9994 - val_loss: 0.0703 - val_acc: 0.9375\n",
      "Epoch 73/100\n",
      "5200/5216 [============================>.] - ETA: 0s - loss: 0.0022 - acc: 0.9998Epoch 00072: val_loss did not improve\n",
      "5216/5216 [==============================] - 6s - loss: 0.0022 - acc: 0.9998 - val_loss: 0.2301 - val_acc: 0.8750\n",
      "Epoch 74/100\n",
      "5200/5216 [============================>.] - ETA: 0s - loss: 6.8378e-04 - acc: 1.0000Epoch 00073: val_loss did not improve\n",
      "5216/5216 [==============================] - 6s - loss: 6.8177e-04 - acc: 1.0000 - val_loss: 0.5934 - val_acc: 0.8750\n",
      "Epoch 75/100\n",
      "5200/5216 [============================>.] - ETA: 0s - loss: 7.2787e-04 - acc: 0.9998Epoch 00074: val_loss did not improve\n",
      "5216/5216 [==============================] - 6s - loss: 7.2566e-04 - acc: 0.9998 - val_loss: 0.2632 - val_acc: 0.8750\n",
      "Epoch 76/100\n",
      "5200/5216 [============================>.] - ETA: 0s - loss: 0.0012 - acc: 0.9998Epoch 00075: val_loss did not improve\n",
      "5216/5216 [==============================] - 6s - loss: 0.0012 - acc: 0.9998 - val_loss: 0.1317 - val_acc: 0.9375\n",
      "Epoch 77/100\n",
      "5200/5216 [============================>.] - ETA: 0s - loss: 0.0087 - acc: 0.9967Epoch 00076: val_loss did not improve\n",
      "5216/5216 [==============================] - 6s - loss: 0.0086 - acc: 0.9967 - val_loss: 0.6178 - val_acc: 0.8750\n",
      "Epoch 78/100\n",
      "5200/5216 [============================>.] - ETA: 0s - loss: 4.5714e-04 - acc: 1.0000Epoch 00077: val_loss did not improve\n",
      "5216/5216 [==============================] - 6s - loss: 4.5574e-04 - acc: 1.0000 - val_loss: 0.2412 - val_acc: 0.9375\n",
      "Epoch 79/100\n",
      "5200/5216 [============================>.] - ETA: 0s - loss: 4.7305e-04 - acc: 0.9998Epoch 00078: val_loss did not improve\n",
      "5216/5216 [==============================] - 6s - loss: 4.7160e-04 - acc: 0.9998 - val_loss: 0.2234 - val_acc: 0.9375\n",
      "Epoch 80/100\n",
      "5200/5216 [============================>.] - ETA: 0s - loss: 1.7946e-04 - acc: 1.0000Epoch 00079: val_loss did not improve\n",
      "5216/5216 [==============================] - 6s - loss: 1.7891e-04 - acc: 1.0000 - val_loss: 0.4452 - val_acc: 0.8750\n",
      "Epoch 81/100\n",
      "5200/5216 [============================>.] - ETA: 0s - loss: 2.6379e-04 - acc: 1.0000Epoch 00080: val_loss did not improve\n",
      "5216/5216 [==============================] - 6s - loss: 2.6298e-04 - acc: 1.0000 - val_loss: 0.8297 - val_acc: 0.8750\n",
      "Epoch 82/100\n",
      "5200/5216 [============================>.] - ETA: 0s - loss: 4.7055e-04 - acc: 0.9998Epoch 00081: val_loss did not improve\n",
      "5216/5216 [==============================] - 6s - loss: 4.6917e-04 - acc: 0.9998 - val_loss: 0.4403 - val_acc: 0.8750\n",
      "Epoch 83/100\n",
      "5200/5216 [============================>.] - ETA: 0s - loss: 7.1407e-04 - acc: 0.9998Epoch 00082: val_loss did not improve\n",
      "5216/5216 [==============================] - 6s - loss: 7.1189e-04 - acc: 0.9998 - val_loss: 0.7380 - val_acc: 0.8750\n",
      "Epoch 84/100\n",
      "5200/5216 [============================>.] - ETA: 0s - loss: 0.0039 - acc: 0.9987Epoch 00083: val_loss did not improve\n",
      "5216/5216 [==============================] - 6s - loss: 0.0039 - acc: 0.9987 - val_loss: 0.3252 - val_acc: 0.8750\n",
      "Epoch 85/100\n",
      "5200/5216 [============================>.] - ETA: 0s - loss: 0.0017 - acc: 0.9996Epoch 00084: val_loss did not improve\n",
      "5216/5216 [==============================] - 6s - loss: 0.0018 - acc: 0.9994 - val_loss: 0.3767 - val_acc: 0.8750\n",
      "Epoch 86/100\n",
      "5200/5216 [============================>.] - ETA: 0s - loss: 8.2815e-04 - acc: 0.9998Epoch 00085: val_loss did not improve\n",
      "5216/5216 [==============================] - 6s - loss: 8.2561e-04 - acc: 0.9998 - val_loss: 0.3496 - val_acc: 0.8750\n",
      "Epoch 87/100\n",
      "5200/5216 [============================>.] - ETA: 0s - loss: 6.4527e-04 - acc: 0.9998Epoch 00086: val_loss did not improve\n",
      "5216/5216 [==============================] - 6s - loss: 6.4329e-04 - acc: 0.9998 - val_loss: 0.2664 - val_acc: 0.9375\n",
      "Epoch 88/100\n",
      "5200/5216 [============================>.] - ETA: 0s - loss: 7.4321e-04 - acc: 0.9998Epoch 00087: val_loss did not improve\n",
      "5216/5216 [==============================] - 6s - loss: 7.4093e-04 - acc: 0.9998 - val_loss: 0.3225 - val_acc: 0.8750\n",
      "Epoch 89/100\n",
      "5200/5216 [============================>.] - ETA: 0s - loss: 0.0017 - acc: 0.9992Epoch 00088: val_loss did not improve\n",
      "5216/5216 [==============================] - 6s - loss: 0.0017 - acc: 0.9992 - val_loss: 0.3681 - val_acc: 0.8750\n",
      "Epoch 90/100\n",
      "5200/5216 [============================>.] - ETA: 0s - loss: 0.0028 - acc: 0.9990Epoch 00089: val_loss did not improve\n",
      "5216/5216 [==============================] - 6s - loss: 0.0028 - acc: 0.9990 - val_loss: 0.3098 - val_acc: 0.8750\n",
      "Epoch 91/100\n",
      "5184/5216 [============================>.] - ETA: 0s - loss: 0.0020 - acc: 0.9992Epoch 00090: val_loss did not improve\n",
      "5216/5216 [==============================] - 6s - loss: 0.0020 - acc: 0.9992 - val_loss: 1.1261 - val_acc: 0.8125\n",
      "Epoch 92/100\n",
      "5200/5216 [============================>.] - ETA: 0s - loss: 0.0010 - acc: 0.9998Epoch 00091: val_loss did not improve\n",
      "5216/5216 [==============================] - 6s - loss: 0.0010 - acc: 0.9998 - val_loss: 0.3635 - val_acc: 0.8750\n",
      "Epoch 93/100\n",
      "5200/5216 [============================>.] - ETA: 0s - loss: 2.2269e-04 - acc: 1.0000Epoch 00092: val_loss did not improve\n",
      "5216/5216 [==============================] - 6s - loss: 2.2236e-04 - acc: 1.0000 - val_loss: 0.4324 - val_acc: 0.8750\n",
      "Epoch 94/100\n",
      "5200/5216 [============================>.] - ETA: 0s - loss: 1.2290e-04 - acc: 1.0000Epoch 00093: val_loss did not improve\n",
      "5216/5216 [==============================] - 6s - loss: 1.2253e-04 - acc: 1.0000 - val_loss: 0.4682 - val_acc: 0.8750\n",
      "Epoch 95/100\n",
      "5200/5216 [============================>.] - ETA: 0s - loss: 1.2371e-04 - acc: 1.0000Epoch 00094: val_loss did not improve\n",
      "5216/5216 [==============================] - 6s - loss: 1.2333e-04 - acc: 1.0000 - val_loss: 0.5053 - val_acc: 0.8750\n",
      "Epoch 96/100\n",
      "5200/5216 [============================>.] - ETA: 0s - loss: 0.0012 - acc: 0.9998Epoch 00095: val_loss did not improve\n",
      "5216/5216 [==============================] - 6s - loss: 0.0012 - acc: 0.9998 - val_loss: 0.1371 - val_acc: 0.9375\n",
      "Epoch 97/100\n",
      "5200/5216 [============================>.] - ETA: 0s - loss: 5.4497e-04 - acc: 1.0000Epoch 00096: val_loss did not improve\n",
      "5216/5216 [==============================] - 6s - loss: 5.4583e-04 - acc: 1.0000 - val_loss: 0.3731 - val_acc: 0.8750\n",
      "Epoch 98/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5200/5216 [============================>.] - ETA: 0s - loss: 3.6153e-04 - acc: 0.9998Epoch 00097: val_loss did not improve\n",
      "5216/5216 [==============================] - 6s - loss: 3.6042e-04 - acc: 0.9998 - val_loss: 0.3835 - val_acc: 0.8750\n",
      "Epoch 99/100\n",
      "5200/5216 [============================>.] - ETA: 0s - loss: 1.5271e-04 - acc: 1.0000Epoch 00098: val_loss did not improve\n",
      "5216/5216 [==============================] - 6s - loss: 1.5232e-04 - acc: 1.0000 - val_loss: 0.3106 - val_acc: 0.8750\n",
      "Epoch 100/100\n",
      "5200/5216 [============================>.] - ETA: 0s - loss: 0.0027 - acc: 0.9994Epoch 00099: val_loss did not improve\n",
      "5216/5216 [==============================] - 6s - loss: 0.0027 - acc: 0.9994 - val_loss: 0.7878 - val_acc: 0.8750\n"
     ]
    }
   ],
   "source": [
    "# fit combined models to data\n",
    "\n",
    "from keras.callbacks import ModelCheckpoint   \n",
    "\n",
    "# train the model\n",
    "checkpointer = ModelCheckpoint(filepath='model.weights.best.hdf5', verbose=1, \n",
    "                               save_best_only=True)\n",
    "\n",
    "predict = model.fit(train_data, train_labels,\n",
    "             epochs=epochs,\n",
    "             batch_size=batch_size,\n",
    "             validation_data=(validation_data, validation_labels), callbacks=[checkpointer], verbose =1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['val_acc', 'acc', 'val_loss', 'loss'])\n"
     ]
    }
   ],
   "source": [
    "print(predict.history.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['val_acc', 'acc', 'val_loss', 'loss'])\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYsAAAEWCAYAAACXGLsWAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f98a6eb94a8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "print(predict.history.keys())\n",
    "plt.plot(predict.history['acc'])\n",
    "plt.plot(predict.history['val_acc'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'val'], loc='lower right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load best trained weights\n",
    "\n",
    "model.load_weights('model.weights.best.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "352/624 [===============>..............] - ETA: 0s\n",
      " Test accuracy: [2.3010393889985754, 0.78685897435897434]\n"
     ]
    }
   ],
   "source": [
    "# print model accuracy on the test data set\n",
    "\n",
    "score = model.evaluate(test_data, test_labels, verbose=1)\n",
    "print('\\n', 'Test accuracy:', score)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
